{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03084f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7ce4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3e8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e38a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LINK='https://files.grouplens.org/datasets/movielens/ml-32m.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677a592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "\n",
    "zip_filename = DATASET_LINK.split('/')[-1]\n",
    "'''This script downloads the MovieLens 32M dataset if it is not already present, and extracts it\n",
    "from the zip file. The dataset is used for building recommendation systems.'''\n",
    "\n",
    "# Download the dataset if not already present\n",
    "if not os.path.exists(zip_filename):\n",
    "\timport ssl\n",
    "\tcontext = ssl._create_unverified_context()\n",
    "\turllib.request.urlretrieve(DATASET_LINK, zip_filename)\n",
    "\n",
    "# Unzip the file if not already extracted\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "\tzip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4474fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    for filename in os.listdir(path):\n",
    "        stem, suffix =  os.path.splitext(filename)\n",
    "        file_path = os.path.join(path,filename)\n",
    "        print(filename)\n",
    "        if suffix == '.csv':\n",
    "            files[stem] = pd.read_csv(file_path)\n",
    "        elif suffix == '.dat':\n",
    "            if stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(file_path, sep='::', names=columns, engine='python')\n",
    "            files[stem] = data\n",
    "    return files['ratings'], files['movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2570e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing data from: /Users/nolanrobbins/Desktop/VS Code Projects/MovieLens-RecSys/data/raw\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/Users/nolanrobbins/Desktop/VS Code Projects/MovieLens-RecSys/data/raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing existing data from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Read data from the raw directory\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m ratings, movies \u001b[38;5;241m=\u001b[39m read_data(extracted_dir)\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mread_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data\u001b[39m(path):\n\u001b[0;32m      2\u001b[0m     files \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path):\n\u001b[0;32m      4\u001b[0m         stem, suffix \u001b[38;5;241m=\u001b[39m  os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)\n\u001b[0;32m      5\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/Users/nolanrobbins/Desktop/VS Code Projects/MovieLens-RecSys/data/raw'"
     ]
    }
   ],
   "source": [
    "# Use existing data in data/raw directory (skip download/extraction)\n",
    "extracted_dir = '/Users/nolanrobbins/Desktop/VS Code Projects/MovieLens-RecSys/data/raw'\n",
    "print(f\"Using existing data from: {extracted_dir}\")\n",
    "\n",
    "# Read data from the raw directory\n",
    "ratings, movies = read_data(extracted_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fc243bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944250228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>943230976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>943228858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1       17     4.0  944249077\n",
       "1       1       25     1.0  944250228\n",
       "2       1       29     2.0  943230976\n",
       "3       1       30     5.0  944249077\n",
       "4       1       32     5.0  943228858"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "291b68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating range: (np.float64(0.5), np.float64(5.0))\n"
     ]
    }
   ],
   "source": [
    "minmax = ratings.rating.min(), ratings.rating.max()\n",
    "print(f\"Rating range: {minmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d62e66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13058938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87585, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cfda9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.merge(movies[[\"movieId\", \"title\"]], on=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "105c4316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944249077</td>\n",
       "      <td>Sense and Sensibility (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944250228</td>\n",
       "      <td>Leaving Las Vegas (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>943230976</td>\n",
       "      <td>City of Lost Children, The (Cité des enfants p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>944249077</td>\n",
       "      <td>Shanghai Triad (Yao a yao yao dao waipo qiao) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>943228858</td>\n",
       "      <td>Twelve Monkeys (a.k.a. 12 Monkeys) (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp  \\\n",
       "0       1       17     4.0  944249077   \n",
       "1       1       25     1.0  944250228   \n",
       "2       1       29     2.0  943230976   \n",
       "3       1       30     5.0  944249077   \n",
       "4       1       32     5.0  943228858   \n",
       "\n",
       "                                               title  \n",
       "0                       Sense and Sensibility (1995)  \n",
       "1                           Leaving Las Vegas (1995)  \n",
       "2  City of Lost Children, The (Cité des enfants p...  \n",
       "3  Shanghai Triad (Yao a yao yao dao waipo qiao) ...  \n",
       "4          Twelve Monkeys (a.k.a. 12 Monkeys) (1995)  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6456ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_preview(ratings, n=15):\n",
    "    \"\"\"Creates a cross-tabular view of users vs movies.\"\"\"\n",
    "    \n",
    "    user_groups = ratings.groupby('userId')['rating'].count()\n",
    "    top_users = user_groups.sort_values(ascending=False)[:n]\n",
    "\n",
    "    movie_groups = ratings.groupby('movieId')['rating'].count()\n",
    "    top_movies = movie_groups.sort_values(ascending=False)[:n]\n",
    "\n",
    "    top = (\n",
    "        ratings.\n",
    "        join(top_users, rsuffix='_r', how='inner', on='userId').\n",
    "        join(top_movies, rsuffix='_r', how='inner', on='movieId'))\n",
    "\n",
    "    return pd.crosstab(top.userId, top.movieId, top.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6cacce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/pvkx_dbx3hn67b1jjpbrc37c0000gn/T/ipykernel_20716/3933855387.py:15: FutureWarning: The provided callable <function sum at 0x1211fade0> is currently using DataFrameGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  return pd.crosstab(top.userId, top.movieId, top.rating, aggfunc=np.sum)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>110</th>\n",
       "      <th>260</th>\n",
       "      <th>296</th>\n",
       "      <th>318</th>\n",
       "      <th>356</th>\n",
       "      <th>480</th>\n",
       "      <th>527</th>\n",
       "      <th>589</th>\n",
       "      <th>593</th>\n",
       "      <th>1196</th>\n",
       "      <th>2571</th>\n",
       "      <th>2959</th>\n",
       "      <th>4993</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14674</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17035</th>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22744</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49305</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55653</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57304</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123465</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129705</th>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133878</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171795</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175325</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198515</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  1     50    110   260   296   318   356   480   527   589   593   \\\n",
       "userId                                                                      \n",
       "7858      5.0   5.0   3.5   4.5   5.0   5.0   2.0   3.0   5.0   3.5   2.5   \n",
       "10202     5.0   5.0   1.0   5.0   5.0   5.0   4.0   4.0   5.0   4.0   5.0   \n",
       "14674     3.0   4.5   5.0   5.0   2.0   2.5   5.0   2.0   3.5   5.0   3.0   \n",
       "17035     3.5   5.0   4.0   0.5   5.0   5.0   3.5   3.0   4.5   3.5   5.0   \n",
       "22744     5.0   4.0   4.0   5.0   5.0   3.0   4.0   5.0   0.5   5.0   5.0   \n",
       "49305     5.0   4.5   4.0   5.0   5.0   4.5   4.5   4.0   5.0   5.0   5.0   \n",
       "53192     NaN   4.0   3.5   5.0   3.5   4.5   4.0   3.0   4.0   3.0   4.0   \n",
       "55653     4.0   4.5   5.0   4.0   4.5   4.0   4.0   3.0   4.0   4.0   4.0   \n",
       "57304     4.0   3.0   NaN   2.5   4.0   2.5   NaN   2.0   5.0   3.0   4.0   \n",
       "123465    5.0   2.5   2.0   3.5   5.0   5.0   4.0   2.0   4.5   3.0   4.0   \n",
       "129705    4.5   5.0   5.0   4.0   5.0   4.0   4.0   4.5   5.0   4.0   4.0   \n",
       "133878    4.0   3.0   4.0   5.0   5.0   5.0   4.0   2.0   5.0   3.5   3.5   \n",
       "171795    3.0   4.0   4.0   4.0   4.0   4.5   3.0   4.0   4.0   4.0   4.0   \n",
       "175325    4.0   4.0   3.5   4.0   4.5   4.5   3.5   4.0   4.5   4.0   4.0   \n",
       "198515    3.0   4.0   4.0   3.0   4.0   4.0   4.0   4.0   4.0   4.0   4.5   \n",
       "\n",
       "movieId  1196  2571  2959  4993  \n",
       "userId                           \n",
       "7858      5.0   5.0   4.0   5.0  \n",
       "10202     5.0   5.0   5.0   5.0  \n",
       "14674     5.0   4.0   3.5   5.0  \n",
       "17035     0.5   1.5   4.5   3.0  \n",
       "22744     5.0   5.0   5.0   5.0  \n",
       "49305     5.0   5.0   4.0   4.5  \n",
       "53192     4.5   2.5   3.0   3.0  \n",
       "55653     4.0   4.0   3.0   4.0  \n",
       "57304     3.0   4.0   3.5   3.0  \n",
       "123465    4.5   2.0   4.5   5.0  \n",
       "129705    4.5   5.0   5.0   5.0  \n",
       "133878    5.0   5.0   5.0   5.0  \n",
       "171795    4.0   4.5   4.0   4.5  \n",
       "175325    4.0   4.0   3.5   4.0  \n",
       "198515    2.5   4.0   2.0   4.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_preview(ratings, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b26553de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "    \n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "    \n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "        \n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "206548ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 200948 users, 84432 movies\n",
      "Dataset shape: (32000204, 2)\n",
      "Target shape: (32000204,)\n"
     ]
    }
   ],
   "source": [
    "(n, m), (X, y), (user_to_index, movie_to_index) = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010a414",
   "metadata": {},
   "source": [
    "# Creating the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1209c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsIterator:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62826ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "369af949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 66953,   5323],\n",
      "        [  9876,   1582],\n",
      "        [ 38347,    968],\n",
      "        [101951,    517]])\n",
      "tensor([[5.],\n",
      "        [4.],\n",
      "        [2.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in batches(X, y, bs=4):\n",
    "    print(x_batch)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f69bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from movielens_past.inter for training (80% chronological past data)\n",
    "print(\"Loading training data from movielens_past.inter...\")\n",
    "\n",
    "# Check if the movielens_past.inter file exists, if not use original approach\n",
    "past_file_path = 'data/processed/movielens_past.inter'\n",
    "if os.path.exists(past_file_path):\n",
    "    # Load from the chronologically split past data\n",
    "    past_ratings_df = pd.read_csv(past_file_path, sep='\\t')\n",
    "    print(f\"Loaded {len(past_ratings_df):,} past interactions from {past_file_path}\")\n",
    "    \n",
    "    # Now split the past data for train/val (80/20 of the past data)\n",
    "    X_past = pd.DataFrame({\n",
    "        'user_id': past_ratings_df['userId'].map(user_to_index), \n",
    "        'movie_id': past_ratings_df['movieId'].map(movie_to_index)\n",
    "    })\n",
    "    y_past = past_ratings_df['rating'].astype(np.float32)\n",
    "    \n",
    "    # Split past data into train/validation\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_past, y_past, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Load future data for ETL pipeline testing\n",
    "    future_file_path = 'data/processed/movielens_future.inter'\n",
    "    if os.path.exists(future_file_path):\n",
    "        future_ratings_df = pd.read_csv(future_file_path, sep='\\t')\n",
    "        X_future = pd.DataFrame({\n",
    "            'user_id': future_ratings_df['userId'].map(user_to_index),\n",
    "            'movie_id': future_ratings_df['movieId'].map(movie_to_index)\n",
    "        })\n",
    "        y_future = future_ratings_df['rating'].astype(np.float32)\n",
    "        \n",
    "        print(f\"Loaded {len(future_ratings_df):,} future interactions for ETL pipeline\")\n",
    "    else:\n",
    "        print(\"Warning: movielens_future.inter not found, using empty future dataset\")\n",
    "        X_future, y_future = pd.DataFrame(), pd.Series(dtype=np.float32)\n",
    "    \n",
    "else:\n",
    "    # Fallback to original timestamp-based split approach\n",
    "    print(\"movielens_past.inter not found, using original timestamp-based split...\")\n",
    "    \n",
    "    # Time-based split: 20% newest data for ETL pipeline testing\n",
    "    timestamp_threshold = ratings['timestamp'].quantile(0.8)\n",
    "    print(f\"Timestamp threshold (80th percentile): {timestamp_threshold}\")\n",
    "\n",
    "    # Split by timestamp\n",
    "    older_data_mask = ratings['timestamp'] <= timestamp_threshold\n",
    "    newer_data_mask = ratings['timestamp'] > timestamp_threshold\n",
    "\n",
    "    # Get indices for X and y\n",
    "    older_indices = X.index[older_data_mask]\n",
    "    newer_indices = X.index[newer_data_mask]\n",
    "\n",
    "    # Create newer dataset (20% most recent)\n",
    "    X_future = X.loc[newer_indices].reset_index(drop=True)\n",
    "    y_future = y.loc[newer_indices].reset_index(drop=True)\n",
    "\n",
    "    # Create older dataset (80% older data)\n",
    "    X_older = X.loc[older_indices].reset_index(drop=True)\n",
    "    y_older = y.loc[older_indices].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Future data (for ETL pipeline): {len(X_future):,} samples\")\n",
    "    print(f\"Older data (for train/val split): {len(X_older):,} samples\")\n",
    "\n",
    "    # Now do normal train/test split on the older data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "       X_older, y_older, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "# Organize datasets\n",
    "datasets = {\n",
    "   'train': (X_train, y_train),\n",
    "   'val': (X_valid, y_valid),\n",
    "   'future': (X_future, y_future)  # For ETL pipeline testing\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "   'train': len(X_train),\n",
    "   'val': len(X_valid),\n",
    "   'future': len(X_future)\n",
    "}\n",
    "\n",
    "print(f\"\\nFinal split sizes:\")\n",
    "print(f\"Training: {dataset_sizes['train']:,}\")\n",
    "print(f\"Validation: {dataset_sizes['val']:,}\")\n",
    "print(f\"Future (ETL): {dataset_sizes['future']:,}\")\n",
    "print(f\"Total: {sum(dataset_sizes.values()):,}\")\n",
    "\n",
    "# Verify data loading\n",
    "print(f\"\\nData verification:\")\n",
    "print(f\"✅ Training data ready from chronologically split past data\")\n",
    "print(f\"✅ Validation data split from training data\")  \n",
    "print(f\"✅ Future data ready for ETL pipeline testing\")\n",
    "print(f\"✅ Using movielens.inter format: userId, movieId, rating, timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf032d9c",
   "metadata": {},
   "source": [
    "# Export Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Generate missing data files for SS4Rec training\n",
    "# First check what data we actually have and create the missing files\n",
    "\n",
    "print(\"🚀 STEP 1: Generating missing data files for SS4Rec training...\")\n",
    "\n",
    "# Check what data files actually exist\n",
    "print(\"🔍 Checking existing data files...\")\n",
    "data_files = {\n",
    "    'raw_data': 'data/raw',\n",
    "    'processed_train': 'data/processed/train_data.csv', \n",
    "    'processed_val': 'data/processed/val_data.csv',\n",
    "    'recbole_inter': 'data/recbole_format/movielens/movielens.inter'\n",
    "}\n",
    "\n",
    "for name, path in data_files.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"✅\" if exists else \"❌\"\n",
    "    print(f\"{status} {name}: {path}\")\n",
    "\n",
    "# Check if we have raw MovieLens data\n",
    "raw_data_dir = 'data/raw'\n",
    "if os.path.exists(raw_data_dir):\n",
    "    print(f\"\\n📁 Raw data directory exists: {raw_data_dir}\")\n",
    "    raw_files = os.listdir(raw_data_dir)\n",
    "    print(f\"📋 Raw files: {raw_files}\")\n",
    "    \n",
    "    # Look for ratings file\n",
    "    ratings_file = None\n",
    "    for file in raw_files:\n",
    "        if 'rating' in file.lower():\n",
    "            ratings_file = os.path.join(raw_data_dir, file)\n",
    "            break\n",
    "    \n",
    "    if ratings_file:\n",
    "        print(f\"✅ Found ratings file: {ratings_file}\")\n",
    "        \n",
    "        # Load and process the raw ratings data\n",
    "        print(\"📊 Loading raw ratings data...\")\n",
    "        if ratings_file.endswith('.dat'):\n",
    "            # MovieLens .dat format\n",
    "            ratings = pd.read_csv(ratings_file, sep='::', names=['userId', 'movieId', 'rating', 'timestamp'], engine='python')\n",
    "        else:\n",
    "            # CSV format\n",
    "            ratings = pd.read_csv(ratings_file)\n",
    "        \n",
    "        print(f\"✅ Loaded {len(ratings):,} raw interactions\")\n",
    "        print(f\"📋 Columns: {list(ratings.columns)}\")\n",
    "        print(\"📋 Sample data:\")\n",
    "        print(ratings.head())\n",
    "        \n",
    "        # Create user and movie mappings\n",
    "        unique_users = ratings['userId'].unique()\n",
    "        unique_movies = ratings['movieId'].unique()\n",
    "        user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "        movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "        \n",
    "        print(f\"📊 Users: {len(unique_users):,}, Movies: {len(unique_movies):,}\")\n",
    "        \n",
    "        # Convert to indexed format\n",
    "        ratings['user_idx'] = ratings['userId'].map(user_to_index)\n",
    "        ratings['movie_idx'] = ratings['movieId'].map(movie_to_index)\n",
    "        \n",
    "        # Time-based split: 80% for training, 20% for future ETL\n",
    "        timestamp_threshold = ratings['timestamp'].quantile(0.8)\n",
    "        print(f\"📅 Split timestamp (80th percentile): {timestamp_threshold}\")\n",
    "        \n",
    "        # Split chronologically\n",
    "        past_mask = ratings['timestamp'] <= timestamp_threshold\n",
    "        future_mask = ratings['timestamp'] > timestamp_threshold\n",
    "        \n",
    "        past_data = ratings[past_mask].copy()\n",
    "        future_data = ratings[future_mask].copy()\n",
    "        \n",
    "        print(f\"📊 Past data (training): {len(past_data):,} interactions ({len(past_data)/len(ratings)*100:.1f}%)\")\n",
    "        print(f\"📊 Future data (ETL pipeline): {len(future_data):,} interactions ({len(future_data)/len(ratings)*100:.1f}%)\")\n",
    "        \n",
    "        # Create processed directory\n",
    "        os.makedirs('data/processed', exist_ok=True)\n",
    "        \n",
    "        # Save processed CSV files\n",
    "        past_data.to_csv('data/processed/train_data.csv', index=False)\n",
    "        future_data.to_csv('data/processed/val_data.csv', index=False)\n",
    "        \n",
    "        print(\"✅ Created train_data.csv and val_data.csv\")\n",
    "        \n",
    "        # Now create RecBole format files\n",
    "        print(\"\\n🔄 Creating RecBole format files...\")\n",
    "        \n",
    "        # Create RecBole directory structure\n",
    "        recbole_dir = 'data/recbole_format/movielens'\n",
    "        os.makedirs(recbole_dir, exist_ok=True)\n",
    "        \n",
    "        # Create the main .inter file (all data for RecBole)\n",
    "        recbole_data = ratings[['user_idx', 'movie_idx', 'rating', 'timestamp']].copy()\n",
    "        recbole_data.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "        \n",
    "        # Sort by user and timestamp (critical for RecBole)\n",
    "        recbole_data = recbole_data.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "        \n",
    "        # Save as .inter file with proper RecBole header\n",
    "        inter_file = os.path.join(recbole_dir, 'movielens.inter')\n",
    "        with open(inter_file, 'w') as f:\n",
    "            # Write RecBole header\n",
    "            f.write('user_id:token\\titem_id:token\\trating:float\\ttimestamp:float\\n')\n",
    "            # Write data\n",
    "            recbole_data.to_csv(f, sep='\\t', index=False, header=False)\n",
    "        \n",
    "        print(f\"✅ Created {inter_file}: {len(recbole_data):,} interactions\")\n",
    "        \n",
    "        # Create chronologically split .inter files for SS4Rec\n",
    "        past_recbole = past_data[['user_idx', 'movie_idx', 'rating', 'timestamp']].copy()\n",
    "        past_recbole.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "        past_recbole = past_recbole.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "        \n",
    "        future_recbole = future_data[['user_idx', 'movie_idx', 'rating', 'timestamp']].copy()\n",
    "        future_recbole.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "        future_recbole = future_recbole.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "        \n",
    "        # Save split files\n",
    "        past_file = 'data/processed/movielens_past.inter'\n",
    "        future_file = 'data/processed/movielens_future.inter'\n",
    "        \n",
    "        with open(past_file, 'w') as f:\n",
    "            f.write('user_id:token\\titem_id:token\\trating:float\\ttimestamp:float\\n')\n",
    "            past_recbole.to_csv(f, sep='\\t', index=False, header=False)\n",
    "        \n",
    "        with open(future_file, 'w') as f:\n",
    "            f.write('user_id:token\\titem_id:token\\trating:float\\ttimestamp:float\\n')\n",
    "            future_recbole.to_csv(f, sep='\\t', index=False, header=False)\n",
    "        \n",
    "        print(f\"✅ Created {past_file}: {len(past_recbole):,} interactions\")\n",
    "        print(f\"✅ Created {future_file}: {len(future_recbole):,} interactions\")\n",
    "        \n",
    "        # Save mappings\n",
    "        import pickle\n",
    "        mappings = {\n",
    "            'user_to_index': user_to_index,\n",
    "            'movie_to_index': movie_to_index,\n",
    "            'n_users': len(unique_users),\n",
    "            'n_movies': len(unique_movies)\n",
    "        }\n",
    "        \n",
    "        with open('data/processed/data_mappings.pkl', 'wb') as f:\n",
    "            pickle.dump(mappings, f)\n",
    "        \n",
    "        print(\"✅ Saved data mappings\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No ratings file found in raw data\")\n",
    "        print(\"💡 Please ensure MovieLens raw data is available in data/raw/\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Raw data directory not found: data/raw\")\n",
    "    print(\"💡 Please download MovieLens data to data/raw/ directory\")\n",
    "\n",
    "print(\"\\n🎯 STEP 1 COMPLETE: Data files generated!\")\n",
    "print(\"✅ train_data.csv - Processed training data\")\n",
    "print(\"✅ val_data.csv - Processed validation data\") \n",
    "print(\"✅ movielens.inter - RecBole format (complete dataset)\")\n",
    "print(\"✅ movielens_past.inter - RecBole format (training data)\")\n",
    "print(\"✅ movielens_future.inter - RecBole format (ETL pipeline data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Verify data schema is correct for RecBole\n",
    "# The data should already be in the correct format, but let's verify\n",
    "\n",
    "print(\"🔧 STEP 2: Verifying data schema for RecBole compatibility...\")\n",
    "\n",
    "# Check the schema of the generated files\n",
    "past_file = 'data/processed/movielens_past.inter'\n",
    "future_file = 'data/processed/movielens_future.inter'\n",
    "\n",
    "if os.path.exists(past_file):\n",
    "    print(f\"📋 Checking schema of {past_file}:\")\n",
    "    past_sample = pd.read_csv(past_file, sep='\\t', nrows=5)\n",
    "    print(f\"Columns: {list(past_sample.columns)}\")\n",
    "    print(\"Sample data:\")\n",
    "    print(past_sample)\n",
    "    \n",
    "    # Verify the schema matches RecBole requirements\n",
    "    expected_columns = ['user_id:token', 'item_id:token', 'rating:float', 'timestamp:float']\n",
    "    actual_columns = list(past_sample.columns)\n",
    "    \n",
    "    if actual_columns == expected_columns:\n",
    "        print(\"✅ Schema is correct for RecBole!\")\n",
    "        print(\"✅ Field names and types match RecBole requirements\")\n",
    "    else:\n",
    "        print(\"❌ Schema mismatch detected!\")\n",
    "        print(f\"Expected: {expected_columns}\")\n",
    "        print(f\"Actual: {actual_columns}\")\n",
    "        \n",
    "        # Fix the schema if needed\n",
    "        print(\"🔧 Fixing schema...\")\n",
    "        past_data = pd.read_csv(past_file, sep='\\t')\n",
    "        future_data = pd.read_csv(future_file, sep='\\t')\n",
    "        \n",
    "        # Rename columns to match RecBole format\n",
    "        column_mapping = {\n",
    "            'userId': 'user_id:token',\n",
    "            'movieId': 'item_id:token', \n",
    "            'rating': 'rating:float',\n",
    "            'timestamp': 'timestamp:float'\n",
    "        }\n",
    "        \n",
    "        # Apply mapping if needed\n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in past_data.columns:\n",
    "                past_data = past_data.rename(columns={old_col: new_col})\n",
    "                future_data = future_data.rename(columns={old_col: new_col})\n",
    "        \n",
    "        # Save corrected files\n",
    "        past_data.to_csv(past_file, sep='\\t', index=False)\n",
    "        future_data.to_csv(future_file, sep='\\t', index=False)\n",
    "        print(\"✅ Schema corrected and files updated!\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ {past_file} not found. Please run Step 1 first.\")\n",
    "\n",
    "print(\"🎯 STEP 2 COMPLETE: Data schema verified and corrected if needed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Download official SS4Rec implementation\n",
    "# This step prepares the official SS4Rec code for integration\n",
    "\n",
    "print(\"📥 STEP 3: Downloading official SS4Rec implementation...\")\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if not os.path.exists('runpod_entrypoint.sh'):\n",
    "    print(\"❌ Not in MovieLens-RecSys directory. Please run this from the project root.\")\n",
    "else:\n",
    "    print(\"✅ In correct directory\")\n",
    "    \n",
    "    # Create a temporary directory for downloading\n",
    "    temp_dir = '/tmp/ss4rec_official'\n",
    "    if os.path.exists(temp_dir):\n",
    "        print(f\"🗑️ Removing existing {temp_dir}\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    try:\n",
    "        # Clone the official SS4Rec repository\n",
    "        print(\"📥 Cloning official SS4Rec repository...\")\n",
    "        result = subprocess.run([\n",
    "            'git', 'clone', \n",
    "            'https://github.com/XiaoWei-i/SS4Rec.git', \n",
    "            temp_dir\n",
    "        ], capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Successfully cloned official SS4Rec repository\")\n",
    "            \n",
    "            # Check what files we got\n",
    "            if os.path.exists(f\"{temp_dir}/sequential_dataset.py\"):\n",
    "                print(\"✅ Found sequential_dataset.py in official implementation\")\n",
    "                \n",
    "                # Copy the official sequential_dataset.py to our project\n",
    "                target_dir = 'models/official_ss4rec'\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                \n",
    "                shutil.copy2(f\"{temp_dir}/sequential_dataset.py\", f\"{target_dir}/sequential_dataset_official.py\")\n",
    "                print(f\"✅ Copied official sequential_dataset.py to {target_dir}/\")\n",
    "                \n",
    "                # Also copy other important files if they exist\n",
    "                important_files = ['SS4Rec.py', 'SS4Rec_sequential.py', 'README.md']\n",
    "                for file in important_files:\n",
    "                    if os.path.exists(f\"{temp_dir}/{file}\"):\n",
    "                        shutil.copy2(f\"{temp_dir}/{file}\", f\"{target_dir}/{file}\")\n",
    "                        print(f\"✅ Copied {file} to {target_dir}/\")\n",
    "                \n",
    "                print(\"🎯 Official SS4Rec files ready for integration!\")\n",
    "                \n",
    "            else:\n",
    "                print(\"❌ sequential_dataset.py not found in official repository\")\n",
    "                print(\"Available files:\")\n",
    "                for item in os.listdir(temp_dir):\n",
    "                    print(f\"  - {item}\")\n",
    "                    \n",
    "        else:\n",
    "            print(f\"❌ Failed to clone repository: {result.stderr}\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"❌ Git clone timed out. Please check your internet connection.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading official SS4Rec: {e}\")\n",
    "    \n",
    "    # Clean up temporary directory\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(\"🗑️ Cleaned up temporary directory\")\n",
    "\n",
    "print(\"🎯 STEP 3 COMPLETE: Official SS4Rec implementation downloaded!\")\n",
    "print(\"📁 Check models/official_ss4rec/ for the official files\")\n",
    "print(\"💡 Next: Integrate official sequential_dataset.py with RecBole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Test training setup and verify success criteria\n",
    "# This step verifies that everything is ready for SS4Rec training\n",
    "\n",
    "print(\"🧪 STEP 4: Testing training setup and verifying success criteria...\")\n",
    "\n",
    "# Check all success criteria from NEXT_STEPS.md\n",
    "success_criteria = {\n",
    "    \"movielens_past.inter exists\": False,\n",
    "    \"movielens_future.inter exists\": False,\n",
    "    \"RecBole dataset loads without errors\": False,\n",
    "    \"Official SS4Rec files available\": False,\n",
    "    \"Training config ready\": False\n",
    "}\n",
    "\n",
    "# Check 1: movielens_past.inter file exists with correct schema\n",
    "past_file = 'data/processed/movielens_past.inter'\n",
    "future_file = 'data/processed/movielens_future.inter'\n",
    "\n",
    "if os.path.exists(past_file):\n",
    "    success_criteria[\"movielens_past.inter exists\"] = True\n",
    "    print(\"✅ movielens_past.inter file exists\")\n",
    "    \n",
    "    # Check file size\n",
    "    file_size = os.path.getsize(past_file) / (1024*1024)  # MB\n",
    "    print(f\"   📊 File size: {file_size:.1f} MB\")\n",
    "    \n",
    "    # Check schema\n",
    "    sample = pd.read_csv(past_file, sep='\\t', nrows=1)\n",
    "    expected_columns = ['user_id:token', 'item_id:token', 'rating:float', 'timestamp:float']\n",
    "    if list(sample.columns) == expected_columns:\n",
    "        print(\"   ✅ Schema is correct\")\n",
    "    else:\n",
    "        print(f\"   ❌ Schema mismatch: {list(sample.columns)}\")\n",
    "else:\n",
    "    print(\"❌ movielens_past.inter file missing\")\n",
    "\n",
    "# Check 2: movielens_future.inter file exists\n",
    "if os.path.exists(future_file):\n",
    "    success_criteria[\"movielens_future.inter exists\"] = True\n",
    "    print(\"✅ movielens_future.inter file exists\")\n",
    "    \n",
    "    file_size = os.path.getsize(future_file) / (1024*1024)  # MB\n",
    "    print(f\"   📊 File size: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(\"❌ movielens_future.inter file missing\")\n",
    "\n",
    "# Check 3: RecBole dataset loads without errors\n",
    "try:\n",
    "    import recbole\n",
    "    print(\"✅ RecBole is available\")\n",
    "    \n",
    "    # Try to load the dataset\n",
    "    from recbole.data import create_dataset\n",
    "    from recbole.config import Config\n",
    "    \n",
    "    # Check if config file exists\n",
    "    config_file = 'configs/official/ss4rec_official.yaml'\n",
    "    if os.path.exists(config_file):\n",
    "        success_criteria[\"Training config ready\"] = True\n",
    "        print(\"✅ Training config file exists\")\n",
    "    else:\n",
    "        print(\"❌ Training config file missing\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ RecBole not available (will be installed on RunPod)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading RecBole: {e}\")\n",
    "\n",
    "# Check 4: Official SS4Rec files available\n",
    "official_dir = 'models/official_ss4rec'\n",
    "if os.path.exists(official_dir):\n",
    "    files = os.listdir(official_dir)\n",
    "    if any('sequential_dataset' in f for f in files):\n",
    "        success_criteria[\"Official SS4Rec files available\"] = True\n",
    "        print(\"✅ Official SS4Rec files available\")\n",
    "        print(f\"   📁 Files: {files}\")\n",
    "    else:\n",
    "        print(\"❌ Official SS4Rec files missing\")\n",
    "else:\n",
    "    print(\"❌ Official SS4Rec directory missing\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 SUCCESS CRITERIA SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "all_passed = True\n",
    "for criterion, passed in success_criteria.items():\n",
    "    status = \"✅\" if passed else \"❌\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"=\"*50)\n",
    "if all_passed:\n",
    "    print(\"🎉 ALL SUCCESS CRITERIA MET!\")\n",
    "    print(\"🚀 Ready for SS4Rec training on RunPod!\")\n",
    "    print(\"💡 Run: ./runpod_entrypoint.sh --model ss4rec-official\")\n",
    "else:\n",
    "    print(\"⚠️  Some criteria not met. Please address the issues above.\")\n",
    "    print(\"💡 Check NEXT_STEPS.md for detailed instructions\")\n",
    "\n",
    "print(\"🎯 STEP 4 COMPLETE: Training setup verification done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export ALL datasets with temporal features for training\n",
    "print(\"Exporting datasets with temporal features...\")\n",
    "\n",
    "# We need to get back to the original ratings indices for each split\n",
    "# The issue is X_train.index doesn't correspond to ratings.index after the splits\n",
    "\n",
    "# Get the original indices from the splits\n",
    "train_mask_indices = older_indices[X_train.index]\n",
    "val_mask_indices = older_indices[X_valid.index] \n",
    "test_mask_indices = newer_indices\n",
    "\n",
    "# Create datasets with full temporal information using original ratings data\n",
    "train_ratings = ratings.iloc[train_mask_indices].copy()\n",
    "val_ratings = ratings.iloc[val_mask_indices].copy()\n",
    "test_ratings = ratings.iloc[test_mask_indices].copy()\n",
    "\n",
    "print(f\"Train ratings shape: {train_ratings.shape}\")\n",
    "print(f\"Val ratings shape: {val_ratings.shape}\")  \n",
    "print(f\"Test ratings shape: {test_ratings.shape}\")\n",
    "\n",
    "# Add user/movie indices to all datasets\n",
    "train_ratings['user_idx'] = train_ratings['userId'].map(user_to_index)\n",
    "train_ratings['movie_idx'] = train_ratings['movieId'].map(movie_to_index)\n",
    "\n",
    "val_ratings['user_idx'] = val_ratings['userId'].map(user_to_index)\n",
    "val_ratings['movie_idx'] = val_ratings['movieId'].map(movie_to_index)\n",
    "\n",
    "test_ratings['user_idx'] = test_ratings['userId'].map(user_to_index)\n",
    "test_ratings['movie_idx'] = test_ratings['movieId'].map(movie_to_index)\n",
    "\n",
    "# Add temporal features to all datasets\n",
    "for name, df in [(\"train\", train_ratings), (\"val\", val_ratings), (\"test\", test_ratings)]:\n",
    "    print(f\"Adding temporal features to {name} data...\")\n",
    "    df['rating_date'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df['rating_year'] = df['rating_date'].dt.year\n",
    "    df['rating_month'] = df['rating_date'].dt.month\n",
    "    df['rating_weekday'] = df['rating_date'].dt.weekday\n",
    "    # Convert to string for CSV compatibility\n",
    "    df['rating_date'] = df['rating_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Define consistent column order\n",
    "columns = ['userId', 'movieId', 'rating', 'timestamp', 'user_idx', 'movie_idx', \n",
    "           'rating_date', 'rating_year', 'rating_month', 'rating_weekday']\n",
    "\n",
    "# Prepare final datasets\n",
    "train_df = train_ratings[columns].copy()\n",
    "val_df = val_ratings[columns].copy()\n",
    "test_df = test_ratings[columns].copy()\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Export to CSV files with temporal features\n",
    "train_df.to_csv('data/processed/train_data.csv', index=False)\n",
    "val_df.to_csv('data/processed/val_data.csv', index=False)\n",
    "test_df.to_csv('data/processed/test_data.csv', index=False)\n",
    "\n",
    "print(f\"✅ Training data exported: {len(train_df):,} samples\")\n",
    "print(f\"✅ Validation data exported: {len(val_df):,} samples\")\n",
    "print(f\"✅ Test data exported: {len(test_df):,} samples\")\n",
    "\n",
    "# Save mappings to processed directory\n",
    "import pickle\n",
    "mappings = {\n",
    "   'user_to_index': user_to_index,\n",
    "   'movie_to_index': movie_to_index,\n",
    "   'n_users': n,\n",
    "   'n_movies': m,\n",
    "   'minmax': minmax\n",
    "}\n",
    "\n",
    "with open('data/processed/data_mappings.pkl', 'wb') as f:\n",
    "    pickle.dump(mappings, f)\n",
    "\n",
    "print(\"✅ Data mappings saved to data/processed/\")\n",
    "print(f\"\\nFinal column format: {list(train_df.columns)}\")\n",
    "print(f\"All datasets now have consistent temporal features!\")\n",
    "print(f\"Users: {n}, Movies: {m}\")\n",
    "\n",
    "# Create movielens.inter file with standard RecBole format\n",
    "print(\"\\nCreating movielens.inter file for RecBole...\")\n",
    "\n",
    "# Combine all data for complete dataset\n",
    "full_dataset = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# Create RecBole format: userId:token, movieId:token, rating:float, timestamp:float\n",
    "recbole_data = full_dataset[['userId', 'movieId', 'rating', 'timestamp']].copy()\n",
    "\n",
    "# Sort by timestamp for proper chronological order\n",
    "recbole_data = recbole_data.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Export as movielens.inter (complete dataset)\n",
    "recbole_data.to_csv('data/processed/movielens.inter', sep='\\t', index=False)\n",
    "\n",
    "print(f\"✅ movielens.inter created with {len(recbole_data):,} interactions\")\n",
    "print(f\"Format: userId\\tmovieId\\trating\\ttimestamp\")\n",
    "\n",
    "# Now split the movielens.inter data chronologically: 80% past / 20% future\n",
    "print(\"\\nSplitting movielens.inter chronologically (80% past / 20% future)...\")\n",
    "\n",
    "# Calculate 80th percentile timestamp for chronological split\n",
    "split_timestamp = recbole_data['timestamp'].quantile(0.8)\n",
    "print(f\"Split timestamp (80th percentile): {split_timestamp}\")\n",
    "\n",
    "# Split chronologically\n",
    "past_data = recbole_data[recbole_data['timestamp'] <= split_timestamp].copy()\n",
    "future_data = recbole_data[recbole_data['timestamp'] > split_timestamp].copy()\n",
    "\n",
    "print(f\"Past data (training): {len(past_data):,} interactions ({len(past_data)/len(recbole_data)*100:.1f}%)\")\n",
    "print(f\"Future data (ETL pipeline): {len(future_data):,} interactions ({len(future_data)/len(recbole_data)*100:.1f}%)\")\n",
    "\n",
    "# Export chronologically split files\n",
    "past_data.to_csv('data/processed/movielens_past.inter', sep='\\t', index=False)\n",
    "future_data.to_csv('data/processed/movielens_future.inter', sep='\\t', index=False)\n",
    "\n",
    "print(f\"✅ movielens_past.inter created: {len(past_data):,} interactions (for SS4Rec training)\")\n",
    "print(f\"✅ movielens_future.inter created: {len(future_data):,} interactions (for ETL pipeline)\")\n",
    "\n",
    "# Verify all files were created correctly\n",
    "files_to_check = [\n",
    "    'data/processed/movielens.inter',\n",
    "    'data/processed/movielens_past.inter', \n",
    "    'data/processed/movielens_future.inter'\n",
    "]\n",
    "\n",
    "print(\"\\nFile verification:\")\n",
    "for filepath in files_to_check:\n",
    "    if os.path.exists(filepath):\n",
    "        file_size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "        print(f\"✅ {filepath}: {file_size:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {filepath} was not created\")\n",
    "\n",
    "print(f\"\\nSample of past data (for training):\")\n",
    "print(past_data.head())\n",
    "print(f\"\\nSample of future data (for ETL):\")\n",
    "print(future_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
