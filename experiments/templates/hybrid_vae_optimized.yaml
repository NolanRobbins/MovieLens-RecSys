# Hybrid VAE Optimized - Based on Real Training Data Analysis
# Designed to push RMSE below 0.55 using proven patterns

metadata:
  name: "hybrid_vae_optimized"
  description: "Data-driven optimization targeting RMSE < 0.55"
  version: "3.0"
  tags: ["hybrid_vae", "optimized", "data_driven", "target_0.55"]
  author: "Deep Learning Engineering Team"
  based_on: "experiment_2 success pattern + advanced optimizations"

# Model architecture - proven depth + strategic scaling
model:
  type: "hybrid_vae"
  architecture:
    n_factors: 200              # Scale up from proven 150
    hidden_dims: [640, 384, 192]  # Proportional scaling of proven [512,256,128]
    latent_dim: 80              # 25% increase from proven 64
    dropout_rate: 0.38          # Slight reduction from proven 0.4
    use_batch_norm: true        # Add stability for larger model
    use_layer_norm: false       # Keep proven simplicity
    activation: "relu"          # Proven activation

# Training configuration - conservative evolution of experiment_2
training:
  # Optimizer - stick with what works
  optimizer: "adam"             # experiment_2 used Adam successfully
  learning_rate: 0.0004         # Slightly lower than proven 0.0005
  weight_decay: 0.000015        # 50% increase from proven 0.00003
  betas: [0.9, 0.999]          # Standard proven values
  
  # Batch configuration - proven sweet spot
  batch_size: 2048              # experiment_2 proven optimal
  max_epochs: 150               # Allow more training than experiment_2's 99
  early_stopping_patience: 20   # More patient than experiment_2's 15
  
  # VAE specific - critical learned values
  kl_weight: 0.008              # Even lower than proven 0.01
  beta_schedule: "cosine"       # Gradual increase vs constant
  beta_min: 0.005               # Start even lower
  beta_max: 0.012               # Cap at proven safe zone
  free_bits: 2.0                # Prevent posterior collapse
  
  # Learning rate scheduling - proven pattern
  lr_scheduler: "reduce_on_plateau"
  lr_factor: 0.5                # Aggressive decay like experiment_2
  lr_patience: 8                # Conservative patience
  min_lr: 0.0000001            # Very low minimum
  
  # Advanced regularization
  gradient_clipping: 1.0        # Prevent instability
  label_smoothing: 0.05         # Gentle regularization
  
# Data configuration - proven setup
data:
  dataset_version: "v1.0"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  min_user_interactions: 20     # Same as successful runs
  min_item_interactions: 5      # Same as successful runs
  rating_threshold: 3.0         # Proven threshold

# Target metrics - aggressive but achievable
targets:
  rmse: 0.52                    # 13% improvement from current best
  precision_at_10: 0.42         # 20% improvement
  ndcg_at_10: 0.48              # 14% improvement
  diversity: 0.58               # Maintain diversity
  coverage: 0.68                # Improve coverage

# Resource requirements
resources:
  gpu_type: "A100"
  memory_gb: 40                 # Increased for larger model
  max_training_hours: 6         # Allow longer training
  mixed_precision: true         # A100 optimization

# Experiment tracking
tracking:
  wandb_project: "movielens-hybrid-vae-optimized"
  log_frequency: 50             # More frequent logging
  save_checkpoints: true
  checkpoint_frequency: 5       # More frequent checkpoints
  
  # Enhanced monitoring
  log_gradients: true           # Monitor for instability
  log_learning_rate: true       # Track LR decay
  log_kl_weight: true          # Track beta scheduling

# Advanced training features
advanced:
  # Stability features
  gradient_accumulation_steps: 1
  warmup_epochs: 5              # Gentle warmup
  swa_start: 0.8                # Stochastic Weight Averaging
  swa_lr: 0.0001                # SWA learning rate
  
  # Model averaging
  ema_decay: 0.999              # Exponential moving average
  
  # Data augmentation
  noise_factor: 0.02            # Gentle input noise
  rating_smoothing: 0.05        # Smooth extreme ratings