{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03084f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7ce4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3e8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e38a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LINK='https://files.grouplens.org/datasets/movielens/ml-32m.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677a592b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This script downloads the MovieLens 32M dataset if it is not already present, and extracts it\\nfrom the zip file. The dataset is used for building recommendation systems.\\n\\n# Download the dataset if not already present\\nif not os.path.exists(zip_filename):\\n\\turllib.request.urlretrieve(DATASET_LINK, zip_filename)\\n\\n# Unzip the file if not already extracted\\nwith zipfile.ZipFile(zip_filename, 'r') as zip_ref:\\n\\tzip_ref.extractall()\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "zip_filename = DATASET_LINK.split('/')[-1]\n",
    "'''This script downloads the MovieLens 32M dataset if it is not already present, and extracts it\n",
    "from the zip file. The dataset is used for building recommendation systems.\n",
    "\n",
    "# Download the dataset if not already present\n",
    "if not os.path.exists(zip_filename):\n",
    "\turllib.request.urlretrieve(DATASET_LINK, zip_filename)\n",
    "\n",
    "# Unzip the file if not already extracted\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "\tzip_ref.extractall()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4474fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    for filename in os.listdir(path):\n",
    "        stem, suffix =  os.path.splitext(filename)\n",
    "        file_path = os.path.join(path,filename)\n",
    "        print(filename)\n",
    "        if suffix == '.csv':\n",
    "            files[stem] = pd.read_csv(file_path)\n",
    "        elif suffix == '.dat':\n",
    "            if stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(file_path, sep='::', names=columns, engine='python')\n",
    "            files[stem] = data\n",
    "    return files['ratings'], files['movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2570e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checksums.txt\n",
      "links.csv\n",
      "movies.csv\n",
      "ratings.csv\n",
      "README.txt\n",
      "tags.csv\n"
     ]
    }
   ],
   "source": [
    "ratings, movies = read_data('ml-32m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc243bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>944250228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>943230976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>944249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>943228858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1       17     4.0  944249077\n",
       "1       1       25     1.0  944250228\n",
       "2       1       29     2.0  943230976\n",
       "3       1       30     5.0  944249077\n",
       "4       1       32     5.0  943228858"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291b68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating range: (0.5, 5.0)\n"
     ]
    }
   ],
   "source": [
    "minmax = ratings.rating.min(), ratings.rating.max()\n",
    "print(f\"Rating range: {minmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d62e66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13058938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87585, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cfda9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.merge(movies[[\"movieId\", \"title\"]], on=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "105c4316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>944249077</td>\n",
       "      <td>Sense and Sensibility (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1084485217</td>\n",
       "      <td>Sense and Sensibility (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1289858271</td>\n",
       "      <td>Sense and Sensibility (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>961513829</td>\n",
       "      <td>Sense and Sensibility (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>845056111</td>\n",
       "      <td>Sense and Sensibility (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp                         title\n",
       "0       1       17     4.0   944249077  Sense and Sensibility (1995)\n",
       "1       3       17     5.0  1084485217  Sense and Sensibility (1995)\n",
       "2      15       17     4.5  1289858271  Sense and Sensibility (1995)\n",
       "3      28       17     4.0   961513829  Sense and Sensibility (1995)\n",
       "4      29       17     4.0   845056111  Sense and Sensibility (1995)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6456ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_preview(ratings, n=15):\n",
    "    \"\"\"Creates a cross-tabular view of users vs movies.\"\"\"\n",
    "    \n",
    "    user_groups = ratings.groupby('userId')['rating'].count()\n",
    "    top_users = user_groups.sort_values(ascending=False)[:n]\n",
    "\n",
    "    movie_groups = ratings.groupby('movieId')['rating'].count()\n",
    "    top_movies = movie_groups.sort_values(ascending=False)[:n]\n",
    "\n",
    "    top = (\n",
    "        ratings.\n",
    "        join(top_users, rsuffix='_r', how='inner', on='userId').\n",
    "        join(top_movies, rsuffix='_r', how='inner', on='movieId'))\n",
    "\n",
    "    return pd.crosstab(top.userId, top.movieId, top.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6cacce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>110</th>\n",
       "      <th>260</th>\n",
       "      <th>296</th>\n",
       "      <th>318</th>\n",
       "      <th>356</th>\n",
       "      <th>480</th>\n",
       "      <th>527</th>\n",
       "      <th>589</th>\n",
       "      <th>593</th>\n",
       "      <th>1196</th>\n",
       "      <th>2571</th>\n",
       "      <th>2959</th>\n",
       "      <th>4993</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14674</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17035</th>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22744</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49305</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55653</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57304</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123465</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129705</th>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133878</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171795</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175325</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198515</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  1     50    110   260   296   318   356   480   527   589   593   \\\n",
       "userId                                                                      \n",
       "7858      5.0   5.0   3.5   4.5   5.0   5.0   2.0   3.0   5.0   3.5   2.5   \n",
       "10202     5.0   5.0   1.0   5.0   5.0   5.0   4.0   4.0   5.0   4.0   5.0   \n",
       "14674     3.0   4.5   5.0   5.0   2.0   2.5   5.0   2.0   3.5   5.0   3.0   \n",
       "17035     3.5   5.0   4.0   0.5   5.0   5.0   3.5   3.0   4.5   3.5   5.0   \n",
       "22744     5.0   4.0   4.0   5.0   5.0   3.0   4.0   5.0   0.5   5.0   5.0   \n",
       "49305     5.0   4.5   4.0   5.0   5.0   4.5   4.5   4.0   5.0   5.0   5.0   \n",
       "53192     NaN   4.0   3.5   5.0   3.5   4.5   4.0   3.0   4.0   3.0   4.0   \n",
       "55653     4.0   4.5   5.0   4.0   4.5   4.0   4.0   3.0   4.0   4.0   4.0   \n",
       "57304     4.0   3.0   NaN   2.5   4.0   2.5   NaN   2.0   5.0   3.0   4.0   \n",
       "123465    5.0   2.5   2.0   3.5   5.0   5.0   4.0   2.0   4.5   3.0   4.0   \n",
       "129705    4.5   5.0   5.0   4.0   5.0   4.0   4.0   4.5   5.0   4.0   4.0   \n",
       "133878    4.0   3.0   4.0   5.0   5.0   5.0   4.0   2.0   5.0   3.5   3.5   \n",
       "171795    3.0   4.0   4.0   4.0   4.0   4.5   3.0   4.0   4.0   4.0   4.0   \n",
       "175325    4.0   4.0   3.5   4.0   4.5   4.5   3.5   4.0   4.5   4.0   4.0   \n",
       "198515    3.0   4.0   4.0   3.0   4.0   4.0   4.0   4.0   4.0   4.0   4.5   \n",
       "\n",
       "movieId  1196  2571  2959  4993  \n",
       "userId                           \n",
       "7858      5.0   5.0   4.0   5.0  \n",
       "10202     5.0   5.0   5.0   5.0  \n",
       "14674     5.0   4.0   3.5   5.0  \n",
       "17035     0.5   1.5   4.5   3.0  \n",
       "22744     5.0   5.0   5.0   5.0  \n",
       "49305     5.0   5.0   4.0   4.5  \n",
       "53192     4.5   2.5   3.0   3.0  \n",
       "55653     4.0   4.0   3.0   4.0  \n",
       "57304     3.0   4.0   3.5   3.0  \n",
       "123465    4.5   2.0   4.5   5.0  \n",
       "129705    4.5   5.0   5.0   5.0  \n",
       "133878    5.0   5.0   5.0   5.0  \n",
       "171795    4.0   4.5   4.0   4.5  \n",
       "175325    4.0   4.0   3.5   4.0  \n",
       "198515    2.5   4.0   2.0   4.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_preview(ratings, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b26553de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "    \n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "    \n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "        \n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "206548ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 200948 users, 84432 movies\n",
      "Dataset shape: (32000204, 2)\n",
      "Target shape: (32000204,)\n"
     ]
    }
   ],
   "source": [
    "(n, m), (X, y), (user_to_index, movie_to_index) = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010a414",
   "metadata": {},
   "source": [
    "# Creating the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1209c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsIterator:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62826ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369af949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42214,   556],\n",
      "        [30845,    65],\n",
      "        [60687,   292],\n",
      "        [ 2000,  1206]])\n",
      "tensor([[3.5000],\n",
      "        [4.0000],\n",
      "        [5.0000],\n",
      "        [2.0000]])\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in batches(X, y, bs=4):\n",
    "    print(x_batch)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99f69bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp threshold (80th percentile): 1538551302.0\n",
      "Newer data (for ETL pipeline): 6,400,041 samples\n",
      "Older data (for train/val split): 25,600,163 samples\n",
      "\n",
      "Final split sizes:\n",
      "Training: 20,480,130\n",
      "Validation: 5,120,033\n",
      "Newer (ETL): 6,400,041\n",
      "Total: 32,000,204\n"
     ]
    }
   ],
   "source": [
    "# Time-based split: 20% newest data for ETL pipeline testing\n",
    "timestamp_threshold = ratings['timestamp'].quantile(0.8)\n",
    "print(f\"Timestamp threshold (80th percentile): {timestamp_threshold}\")\n",
    "\n",
    "# Split by timestamp\n",
    "older_data_mask = ratings['timestamp'] <= timestamp_threshold\n",
    "newer_data_mask = ratings['timestamp'] > timestamp_threshold\n",
    "\n",
    "# Get indices for X and y\n",
    "older_indices = X.index[older_data_mask]\n",
    "newer_indices = X.index[newer_data_mask]\n",
    "\n",
    "# Create newer dataset (20% most recent)\n",
    "X_newer = X.loc[newer_indices].reset_index(drop=True)\n",
    "y_newer = y.loc[newer_indices].reset_index(drop=True)\n",
    "\n",
    "# Create older dataset (80% older data)\n",
    "X_older = X.loc[older_indices].reset_index(drop=True)\n",
    "y_older = y.loc[older_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"Newer data (for ETL pipeline): {len(X_newer):,} samples\")\n",
    "print(f\"Older data (for train/val split): {len(X_older):,} samples\")\n",
    "\n",
    "# Now do normal train/test split on the older data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "   X_older, y_older, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Organize datasets\n",
    "datasets = {\n",
    "   'train': (X_train, y_train),\n",
    "   'val': (X_valid, y_valid),\n",
    "   'newer': (X_newer, y_newer)  # For ETL pipeline testing\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "'train': len(X_train),\n",
    "   'val': len(X_valid),\n",
    "   'newer': len(X_newer)\n",
    "}\n",
    "\n",
    "print(f\"\\nFinal split sizes:\")\n",
    "print(f\"Training: {dataset_sizes['train']:,}\")\n",
    "print(f\"Validation: {dataset_sizes['val']:,}\")\n",
    "print(f\"Newer (ETL): {dataset_sizes['newer']:,}\")\n",
    "print(f\"Total: {sum(dataset_sizes.values()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e928d0",
   "metadata": {},
   "source": [
    "# Building our RecSys Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "      class HybridVAE(nn.Module):\n",
    "          \"\"\"\n",
    "          Hybrid Variational \n",
    "      Autoencoder + Embedding Network\n",
    "       for Recommendation Systems\n",
    "          \n",
    "          Combines collaborative \n",
    "      filtering embeddings with \n",
    "      variational autoencoder\n",
    "          to capture both user-item \n",
    "      interactions and generate new \n",
    "      recommendations.\n",
    "          \n",
    "          Args:\n",
    "              n_users: Number of \n",
    "      unique users\n",
    "              n_movies: Number of \n",
    "      unique movies  \n",
    "              n_factors: Embedding \n",
    "      dimension\n",
    "              hidden_dims: List of \n",
    "      hidden layer dimensions for \n",
    "      encoder/decoder\n",
    "              latent_dim: Latent \n",
    "      space dimension\n",
    "              dropout_rate: Dropout \n",
    "      rate for regularization\n",
    "          \"\"\"\n",
    "\n",
    "          def __init__(self, n_users,\n",
    "       n_movies, n_factors=150, \n",
    "      \n",
    "      hidden_dims=[512, 256], \n",
    "      latent_dim=64, \n",
    "      \n",
    "      dropout_rate=0.3):\n",
    "              super(HybridVAE,\n",
    "      self).__init__()\n",
    "\n",
    "              # Embedding layers \n",
    "      (from original EmbeddingNet)\n",
    "              self.user_embedding =\n",
    "      nn.Embedding(n_users,\n",
    "      n_factors)\n",
    "              self.movie_embedding =\n",
    "      nn.Embedding(n_movies,\n",
    "      n_factors)\n",
    "              self.embedding_dropout\n",
    "      = nn.Dropout(dropout_rate *\n",
    "      0.5)  # Lower dropout for \n",
    "      embeddings\n",
    "\n",
    "              # Encoder network\n",
    "              encoder_layers = []\n",
    "              input_dim = n_factors *\n",
    "       2  # Concatenated user + movie\n",
    "       embeddings\n",
    "\n",
    "              for hidden_dim in\n",
    "      hidden_dims:\n",
    "\n",
    "      encoder_layers.extend([\n",
    "\n",
    "      nn.Linear(input_dim,\n",
    "      hidden_dim),\n",
    "\n",
    "      nn.BatchNorm1d(hidden_dim),\n",
    "                      nn.ReLU(),\n",
    "\n",
    "      nn.Dropout(dropout_rate)\n",
    "                  ])\n",
    "                  input_dim =\n",
    "      hidden_dim\n",
    "\n",
    "              self.encoder =\n",
    "      nn.Sequential(*encoder_layers)\n",
    "\n",
    "              # VAE latent space\n",
    "              self.mu_layer =\n",
    "      nn.Linear(hidden_dims[-1],\n",
    "      latent_dim)\n",
    "              self.logvar_layer =\n",
    "      nn.Linear(hidden_dims[-1],\n",
    "      latent_dim)\n",
    "\n",
    "              # Decoder network  \n",
    "              decoder_layers = []\n",
    "              input_dim = latent_dim\n",
    "\n",
    "              for hidden_dim in\n",
    "      reversed(hidden_dims):\n",
    "\n",
    "      decoder_layers.extend([\n",
    "\n",
    "      nn.Linear(input_dim,\n",
    "      hidden_dim),\n",
    "\n",
    "      nn.BatchNorm1d(hidden_dim),\n",
    "                      nn.ReLU(),\n",
    "\n",
    "      nn.Dropout(dropout_rate)\n",
    "                  ])\n",
    "                  input_dim =\n",
    "      hidden_dim\n",
    "\n",
    "              # Final output layer\n",
    "              decoder_layers.append(n\n",
    "      n.Linear(input_dim, 1))\n",
    "              self.decoder =\n",
    "      nn.Sequential(*decoder_layers)\n",
    "\n",
    "              # Initialize weights\n",
    "              self._init_weights()\n",
    "\n",
    "          def _init_weights(self):\n",
    "              \"\"\"Initialize weights \n",
    "      with proper scaling\"\"\"\n",
    "              # Embedding \n",
    "      initialization\n",
    "              nn.init.uniform_(self.u\n",
    "      ser_embedding.weight, -0.05,\n",
    "      0.05)\n",
    "              nn.init.uniform_(self.m\n",
    "      ovie_embedding.weight, -0.05,\n",
    "      0.05)\n",
    "\n",
    "              # Xavier initialization\n",
    "       for linear layers\n",
    "              for module in\n",
    "      self.modules():\n",
    "                  if\n",
    "      isinstance(module, nn.Linear):\n",
    "                      nn.init.xavier_\n",
    "      uniform_(module.weight)\n",
    "                      if module.bias\n",
    "      is not None:\n",
    "\n",
    "      nn.init.constant_(module.bias,\n",
    "      0.01)\n",
    "\n",
    "          def encode(self, users, \n",
    "      movies):\n",
    "              \"\"\"Encode user-movie \n",
    "      pairs to latent space\"\"\"\n",
    "              # Get embeddings\n",
    "              user_emb =\n",
    "      self.user_embedding(users)\n",
    "              movie_emb =\n",
    "      self.movie_embedding(movies)\n",
    "\n",
    "              # Concatenate and apply\n",
    "       dropout\n",
    "              features =\n",
    "      torch.cat([user_emb,\n",
    "      movie_emb], dim=1)\n",
    "              features = self.embeddi\n",
    "      ng_dropout(features)\n",
    "\n",
    "              # Pass through encoder\n",
    "              encoded =\n",
    "      self.encoder(features)\n",
    "\n",
    "              # Get mean and log \n",
    "      variance for latent space\n",
    "              mu =\n",
    "      self.mu_layer(encoded)\n",
    "              logvar =\n",
    "      self.logvar_layer(encoded)\n",
    "\n",
    "              return mu, logvar\n",
    "\n",
    "          def reparameterize(self, \n",
    "      mu, logvar):\n",
    "              \"\"\"Reparameterization \n",
    "      trick for VAE\"\"\"\n",
    "              if self.training:\n",
    "                  std = torch.exp(0.5\n",
    "       * logvar)\n",
    "                  eps =\n",
    "      torch.randn_like(std)\n",
    "                  return mu + eps *\n",
    "      std\n",
    "              else:\n",
    "                  return mu  # Use \n",
    "      mean during inference\n",
    "\n",
    "          def decode(self, z):\n",
    "              \"\"\"Decode latent \n",
    "      representation to rating \n",
    "      prediction\"\"\"\n",
    "              decoded =\n",
    "      self.decoder(z)\n",
    "              return\n",
    "      torch.sigmoid(decoded)  # \n",
    "      Sigmoid for rating \n",
    "      normalization\n",
    "\n",
    "          def forward(self, users, \n",
    "      movies, minmax=None):\n",
    "              \"\"\"Forward pass through\n",
    "       hybrid VAE\"\"\"\n",
    "              # Encode to latent \n",
    "      space\n",
    "              mu, logvar =\n",
    "      self.encode(users, movies)\n",
    "\n",
    "              # Reparameterize\n",
    "              z =\n",
    "      self.reparameterize(mu, logvar)\n",
    "\n",
    "              # Decode to rating \n",
    "      prediction\n",
    "              rating_pred =\n",
    "      self.decode(z)\n",
    "\n",
    "              # Scale to rating range\n",
    "       if provided\n",
    "              if minmax is not None:\n",
    "                  min_rating,\n",
    "      max_rating = minmax\n",
    "                  rating_pred =\n",
    "      rating_pred * (max_rating -\n",
    "      min_rating) + min_rating\n",
    "\n",
    "              return rating_pred, mu,\n",
    "       logvar\n",
    "\n",
    "          def \n",
    "      generate_recommendations(self, \n",
    "      user_ids, n_recommendations=10,\n",
    "       \n",
    "      \n",
    "      movie_candidates=None, \n",
    "      minmax=None):\n",
    "              \"\"\"Generate \n",
    "      recommendations for given \n",
    "      users\"\"\"\n",
    "              self.eval()\n",
    "              recommendations = []\n",
    "\n",
    "              with torch.no_grad():\n",
    "                  for user_id in\n",
    "      user_ids:\n",
    "                      if\n",
    "      movie_candidates is None:\n",
    "                          # Use all \n",
    "      movies if not specified\n",
    "\n",
    "      movie_candidates =\n",
    "      torch.arange(self.movie_embeddi\n",
    "      ng.num_embeddings)\n",
    "\n",
    "                      # Create \n",
    "      user-movie pairs\n",
    "                      users = torch.f\n",
    "      ull((len(movie_candidates),),\n",
    "      user_id, dtype=torch.long)\n",
    "                      movies =\n",
    "      movie_candidates\n",
    "\n",
    "                      # Get \n",
    "      predictions\n",
    "                      ratings, _, _ =\n",
    "       self.forward(users, movies,\n",
    "      minmax)\n",
    "\n",
    "                      # Get top \n",
    "      recommendations\n",
    "                      _, top_indices\n",
    "      = torch.topk(ratings.squeeze(),\n",
    "       n_recommendations)\n",
    "                      top_movies =\n",
    "      movie_candidates[top_indices]\n",
    "                      top_ratings =\n",
    "      ratings.squeeze()[top_indices]\n",
    "\n",
    "\n",
    "      recommendations.append({\n",
    "                          'user_id':\n",
    "      user_id,\n",
    "\n",
    "      'recommended_movies':\n",
    "      top_movies.cpu().numpy(),\n",
    "\n",
    "      'predicted_ratings':\n",
    "      top_ratings.cpu().numpy()\n",
    "                      })\n",
    "\n",
    "              return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def \n",
    "      vae_loss_function(predictions, \n",
    "      targets, mu, logvar, \n",
    "      kl_weight=1.0):\n",
    "          \"\"\"\n",
    "          VAE loss combining \n",
    "      reconstruction loss and KL \n",
    "      divergence\n",
    "          \n",
    "          Args:\n",
    "              predictions: Model \n",
    "      predictions\n",
    "              targets: Ground truth \n",
    "      ratings\n",
    "              mu: Mean of latent \n",
    "      distribution\n",
    "              logvar: Log variance of\n",
    "       latent distribution\n",
    "              kl_weight: Weight for \n",
    "      KL divergence term (beta-VAE)\n",
    "          \"\"\"\n",
    "          # Reconstruction loss (MSE \n",
    "      for ratings)\n",
    "          recon_loss = F.mse_loss(pre\n",
    "      dictions.squeeze(), targets,\n",
    "      reduction='sum')\n",
    "\n",
    "          # KL divergence loss\n",
    "          kl_loss = -0.5 *\n",
    "      torch.sum(1 + logvar -\n",
    "      mu.pow(2) - logvar.exp())\n",
    "\n",
    "          # Total loss\n",
    "          total_loss = recon_loss +\n",
    "      kl_weight * kl_loss\n",
    "\n",
    "          return total_loss,\n",
    "      recon_loss, kl_loss\n",
    "\n",
    "      def train_hybrid_vae(model, \n",
    "      train_loader, val_loader, \n",
    "      n_epochs=100, \n",
    "                           lr=1e-4, \n",
    "      weight_decay=1e-5, \n",
    "      device='cuda',\n",
    "      \n",
    "      patience=15, kl_weight=1.0, \n",
    "      minmax=(0.5, 5.0)):\n",
    "          \"\"\"\n",
    "          Training loop for Hybrid \n",
    "      VAE model\n",
    "          \"\"\"\n",
    "          model = model.to(device)\n",
    "          optimizer =\n",
    "      optim.Adam(model.parameters(),\n",
    "      lr=lr,\n",
    "      weight_decay=weight_decay)\n",
    "          scheduler =\n",
    "      optim.lr_scheduler.ReduceLROnPl\n",
    "      ateau(optimizer, 'min',\n",
    "      patience=5, factor=0.5)\n",
    "\n",
    "          best_val_loss =\n",
    "      float('inf')\n",
    "          patience_counter = 0\n",
    "          best_weights = None\n",
    "\n",
    "          history = {\n",
    "              'train_loss': [],\n",
    "      'val_loss': [], 'train_recon':\n",
    "      [],\n",
    "              'val_recon': [],\n",
    "      'train_kl': [], 'val_kl': []\n",
    "          }\n",
    "\n",
    "          for epoch in\n",
    "      range(n_epochs):\n",
    "              # Training phase\n",
    "              model.train()\n",
    "              train_total_loss = 0.0\n",
    "              train_recon_loss = 0.0\n",
    "              train_kl_loss = 0.0\n",
    "              train_batches = 0\n",
    "\n",
    "              for batch_x, batch_y in\n",
    "       train_loader:\n",
    "                  batch_x, batch_y =\n",
    "      batch_x.to(device),\n",
    "      batch_y.to(device)\n",
    "\n",
    "                  # Forward pass\n",
    "                  predictions, mu,\n",
    "      logvar = model(batch_x[:, 0],\n",
    "      batch_x[:, 1], minmax)\n",
    "\n",
    "                  # Calculate loss\n",
    "                  total_loss,\n",
    "      recon_loss, kl_loss =\n",
    "      vae_loss_function(\n",
    "                      predictions,\n",
    "      batch_y.squeeze(), mu, logvar,\n",
    "      kl_weight\n",
    "                  )\n",
    "\n",
    "                  # Backward pass\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      total_loss.backward()\n",
    "                  torch.nn.utils.clip\n",
    "      _grad_norm_(model.parameters(),\n",
    "       max_norm=1.0)\n",
    "                  optimizer.step()\n",
    "\n",
    "                  # Accumulate losses\n",
    "                  batch_size =\n",
    "      batch_x.size(0)\n",
    "                  train_total_loss +=\n",
    "       total_loss.item()\n",
    "                  train_recon_loss +=\n",
    "       recon_loss.item()\n",
    "                  train_kl_loss +=\n",
    "      kl_loss.item()\n",
    "                  train_batches += 1\n",
    "\n",
    "              # Validation phase\n",
    "              model.eval()\n",
    "              val_total_loss = 0.0\n",
    "              val_recon_loss = 0.0\n",
    "              val_kl_loss = 0.0\n",
    "              val_batches = 0\n",
    "\n",
    "              with torch.no_grad():\n",
    "                  for batch_x,\n",
    "      batch_y in val_loader:\n",
    "                      batch_x,\n",
    "      batch_y = batch_x.to(device),\n",
    "      batch_y.to(device)\n",
    "\n",
    "                      predictions,\n",
    "      mu, logvar = model(batch_x[:,\n",
    "      0], batch_x[:, 1], minmax)\n",
    "                      total_loss,\n",
    "      recon_loss, kl_loss =\n",
    "      vae_loss_function(\n",
    "\n",
    "      predictions, batch_y.squeeze(),\n",
    "       mu, logvar, kl_weight\n",
    "                      )\n",
    "\n",
    "                      val_total_loss\n",
    "      += total_loss.item()\n",
    "                      val_recon_loss\n",
    "      += recon_loss.item()\n",
    "                      val_kl_loss +=\n",
    "      kl_loss.item()\n",
    "                      val_batches +=\n",
    "      1\n",
    "\n",
    "              # Average losses\n",
    "              avg_train_loss =\n",
    "      train_total_loss /\n",
    "      train_batches\n",
    "              avg_val_loss =\n",
    "      val_total_loss / val_batches\n",
    "              avg_train_recon =\n",
    "      train_recon_loss /\n",
    "      train_batches\n",
    "              avg_val_recon =\n",
    "      val_recon_loss / val_batches\n",
    "              avg_train_kl =\n",
    "      train_kl_loss / train_batches\n",
    "              avg_val_kl =\n",
    "      val_kl_loss / val_batches\n",
    "\n",
    "              # Update history\n",
    "              history['train_loss'].a\n",
    "      ppend(avg_train_loss)\n",
    "              history['val_loss'].app\n",
    "      end(avg_val_loss)\n",
    "              history['train_recon'].\n",
    "      append(avg_train_recon)\n",
    "              history['val_recon'].ap\n",
    "      pend(avg_val_recon)\n",
    "              history['train_kl'].app\n",
    "      end(avg_train_kl)\n",
    "              history['val_kl'].appen\n",
    "      d(avg_val_kl)\n",
    "\n",
    "              # Learning rate \n",
    "      scheduling\n",
    "\n",
    "      scheduler.step(avg_val_loss)\n",
    "\n",
    "              # Early stopping\n",
    "              if avg_val_loss <\n",
    "      best_val_loss:\n",
    "                  best_val_loss =\n",
    "      avg_val_loss\n",
    "                  best_weights = copy\n",
    "      .deepcopy(model.state_dict())\n",
    "                  patience_counter =\n",
    "      0\n",
    "                  print(f'✓ Epoch \n",
    "      {epoch+1:03d}: Val loss \n",
    "      improved to \n",
    "      {avg_val_loss:.4f}')\n",
    "              else:\n",
    "                  patience_counter +=\n",
    "       1\n",
    "\n",
    "              # Print progress\n",
    "              if (epoch + 1) % 10 ==\n",
    "      0 or epoch == 0:\n",
    "                  print(f'Epoch \n",
    "      {epoch+1:03d}/{n_epochs:03d} | \n",
    "      '\n",
    "                        f'Train: \n",
    "      {avg_train_loss:.4f} | Val: \n",
    "      {avg_val_loss:.4f} | '\n",
    "                        f'Recon: \n",
    "      {avg_val_recon:.4f} | KL: \n",
    "      {avg_val_kl:.4f} | '\n",
    "                        f'LR: \n",
    "      {optimizer.param_groups[0][\"lr\"\n",
    "      ]:.2e}')\n",
    "\n",
    "              # Early stopping check\n",
    "              if patience_counter >=\n",
    "      patience:\n",
    "                  print(f'Early \n",
    "      stopping at epoch {epoch+1}')\n",
    "                  break\n",
    "\n",
    "          # Load best weights\n",
    "          if best_weights is not\n",
    "      None:\n",
    "              model.load_state_dict(b\n",
    "      est_weights)\n",
    "\n",
    "          return model, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
