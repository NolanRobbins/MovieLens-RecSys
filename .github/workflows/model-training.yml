name: Model Training Pipeline

on:
  workflow_dispatch:
    inputs:
      model_types:
        description: 'Models to train (comma-separated: matrix_factorization,neural_cf,two_tower,hybrid_vae)'
        required: false
        default: 'matrix_factorization,hybrid_vae'
      epochs:
        description: 'Number of training epochs'
        required: false
        default: '20'
      force_retrain:
        description: 'Force retraining even if recent model exists'
        type: boolean
        default: false
  repository_dispatch:
    types: [model_drift_detected]

env:
  PYTHON_VERSION: 3.9

jobs:
  validate-data:
    runs-on: ubuntu-latest
    outputs:
      data_quality_score: ${{ steps.quality_check.outputs.score }}
      should_proceed: ${{ steps.quality_check.outputs.should_proceed }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Check data availability
      run: |
        python -c "
        from pathlib import Path
        import sys
        
        # Check for required data files
        required_files = [
            'data/processed/train_data.csv',
            'data/processed/val_data.csv', 
            'data/processed/movies_processed.csv'
        ]
        
        missing_files = []
        for file_path in required_files:
            if not Path(file_path).exists():
                missing_files.append(file_path)
        
        if missing_files:
            print(f'‚ùå Missing required files: {missing_files}')
            print('Please ensure data preprocessing is complete')
            sys.exit(1)
        else:
            print('‚úÖ All required data files present')
        "
    
    - name: Data quality validation
      id: quality_check
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        import sys
        
        try:
            # Load and validate training data
            train_data = pd.read_csv('data/processed/train_data.csv')
            val_data = pd.read_csv('data/processed/val_data.csv')
            
            # Basic quality checks
            train_quality = {
                'missing_values': train_data.isnull().sum().sum(),
                'duplicate_rows': train_data.duplicated().sum(),
                'invalid_ratings': len(train_data[(train_data['rating'] < 0.5) | (train_data['rating'] > 5.0)]),
                'unique_users': train_data['user_id'].nunique(),
                'unique_movies': train_data['movie_id'].nunique(),
                'total_interactions': len(train_data)
            }
            
            # Calculate quality score
            quality_score = 1.0
            quality_score -= min(0.3, train_quality['missing_values'] / len(train_data))
            quality_score -= min(0.2, train_quality['duplicate_rows'] / len(train_data))
            quality_score -= min(0.2, train_quality['invalid_ratings'] / len(train_data))
            
            print(f'Data Quality Report:')
            print(f'  Total interactions: {train_quality[\"total_interactions\"]:,}')
            print(f'  Unique users: {train_quality[\"unique_users\"]:,}')
            print(f'  Unique movies: {train_quality[\"unique_movies\"]:,}')
            print(f'  Missing values: {train_quality[\"missing_values\"]}')
            print(f'  Duplicate rows: {train_quality[\"duplicate_rows\"]}')
            print(f'  Invalid ratings: {train_quality[\"invalid_ratings\"]}')
            print(f'  Quality score: {quality_score:.3f}')
            
            # Set outputs
            with open('$GITHUB_OUTPUT', 'a') as f:
                f.write(f'score={quality_score:.3f}\\n')
                f.write(f'should_proceed={str(quality_score >= 0.8).lower()}\\n')
            
            if quality_score < 0.8:
                print('‚ùå Data quality below threshold (0.8)')
                sys.exit(1)
            else:
                print('‚úÖ Data quality acceptable for training')
                
        except Exception as e:
            print(f'‚ùå Data validation failed: {e}')
            sys.exit(1)
        "

  train-models:
    runs-on: ubuntu-latest
    needs: validate-data
    if: needs.validate-data.outputs.should_proceed == 'true'
    strategy:
      matrix:
        # Dynamic matrix based on input, but fallback to default models
        model_type: ${{ fromJson('["matrix_factorization", "hybrid_vae"]') }}
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
    
    - name: Check for existing model
      id: model_check
      run: |
        MODEL_PATH="data/models/${{ matrix.model_type }}_latest.pt"
        if [ -f "$MODEL_PATH" ] && [ "${{ github.event.inputs.force_retrain }}" != "true" ]; then
          echo "Model exists and force_retrain is false"
          echo "skip_training=true" >> $GITHUB_OUTPUT
        else
          echo "Training required"
          echo "skip_training=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Create models directory
      run: mkdir -p data/models data/experiments
    
    - name: Train model - ${{ matrix.model_type }}
      if: steps.model_check.outputs.skip_training == 'false'
      run: |
        python -c "
        import sys
        import time
        import torch
        import pandas as pd
        from datetime import datetime
        sys.path.append('src')
        
        from models.baseline_models import ModelFactory, count_parameters
        from models.experiment_runner import ModelTrainer, ExperimentConfig
        
        print(f'üöÄ Training {\"${{ matrix.model_type }}\"}...')
        start_time = time.time()
        
        try:
            # Load data dimensions
            train_data = pd.read_csv('data/processed/train_data.csv')
            n_users = train_data['user_id'].nunique() + 1
            n_movies = train_data['movie_id'].nunique() + 1
            
            print(f'Data: {len(train_data):,} interactions, {n_users:,} users, {n_movies:,} movies')
            
            # Create model config
            model_config = {
                'name': '${{ matrix.model_type }}',
                'type': '${{ matrix.model_type }}',
                'params': {}
            }
            
            # Model-specific parameters
            if '${{ matrix.model_type }}' == 'matrix_factorization':
                model_config['params'] = {'n_factors': 128, 'dropout_rate': 0.2}
            elif '${{ matrix.model_type }}' == 'neural_cf':
                model_config['params'] = {'mf_dim': 64, 'mlp_dims': [128, 64, 32]}
            elif '${{ matrix.model_type }}' == 'two_tower':
                model_config['params'] = {'embedding_dim': 128, 'tower_dims': [256, 128, 64]}
            elif '${{ matrix.model_type }}' == 'hybrid_vae':
                model_config['params'] = {'n_factors': 150, 'hidden_dims': [512, 256], 'latent_dim': 64}
            
            # Create trainer with limited epochs for CI
            config = ExperimentConfig()
            config.epochs = min(int('${{ github.event.inputs.epochs }}' or '20'), 30)  # Limit for CI
            config.batch_size = 1024  # Larger batches for faster training
            
            trainer = ModelTrainer(config)
            train_loader, val_loader, mappings = trainer.load_data()
            
            # Train model
            result = trainer.train_model(model_config, train_loader, val_loader, n_users, n_movies)
            
            # Save training results
            training_time = time.time() - start_time
            
            results = {
                'model_type': '${{ matrix.model_type }}',
                'training_time': training_time,
                'best_val_loss': result['best_val_loss'],
                'param_count': result['param_count'],
                'training_date': datetime.now().isoformat(),
                'github_sha': '${{ github.sha }}',
                'epochs_trained': len(result['train_losses']),
                'data_quality_score': float('${{ needs.validate-data.outputs.data_quality_score }}')
            }
            
            # Save model with metadata
            torch.save({
                'model_state_dict': result['model'].state_dict(),
                'model_config': model_config,
                'training_results': results,
                'data_mappings': {'n_users': n_users, 'n_movies': n_movies}
            }, f'data/models/{\"${{ matrix.model_type }}\"}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pt')
            
            # Also save as latest
            torch.save({
                'model_state_dict': result['model'].state_dict(),
                'model_config': model_config,
                'training_results': results,
                'data_mappings': {'n_users': n_users, 'n_movies': n_movies}
            }, f'data/models/{\"${{ matrix.model_type }}\"}__latest.pt')
            
            print(f'‚úÖ Model training completed successfully!')
            print(f'   Training time: {training_time:.1f}s')
            print(f'   Best validation loss: {result[\"best_val_loss\"]:.4f}')
            print(f'   Parameters: {result[\"param_count\"]:,}')
            
        except Exception as e:
            print(f'‚ùå Training failed: {e}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        "
    
    - name: Upload model artifacts
      if: steps.model_check.outputs.skip_training == 'false'
      uses: actions/upload-artifact@v3
      with:
        name: model-${{ matrix.model_type }}
        path: data/models/${{ matrix.model_type }}_*.pt
        retention-days: 30

  evaluate-models:
    runs-on: ubuntu-latest
    needs: [train-models, validate-data]
    if: always() && needs.validate-data.outputs.should_proceed == 'true'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
    
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        path: downloaded-models
    
    - name: Copy models to correct location
      run: |
        mkdir -p data/models
        if [ -d "downloaded-models" ]; then
          find downloaded-models -name "*.pt" -exec cp {} data/models/ \;
          echo "Downloaded models:"
          ls -la data/models/
        fi
    
    - name: Evaluate models and compare performance
      run: |
        python -c "
        import sys
        import torch
        import pandas as pd
        import json
        from pathlib import Path
        from datetime import datetime
        sys.path.append('src')
        
        from business.business_metrics import BusinessMetricsTracker, MLMetricsConverter
        
        print('üî¨ Evaluating trained models...')
        
        # Find trained models
        model_files = list(Path('data/models').glob('*_latest.pt'))
        
        if not model_files:
            print('‚ö†Ô∏è No models found for evaluation')
            sys.exit(0)
        
        evaluation_results = {}
        tracker = BusinessMetricsTracker()
        
        for model_file in model_files:
            try:
                model_name = model_file.stem.replace('_latest', '')
                print(f'\\nüìä Evaluating {model_name}...')
                
                # Load model checkpoint
                checkpoint = torch.load(model_file, map_location='cpu')
                training_results = checkpoint.get('training_results', {})
                
                # Mock evaluation metrics (in production, run actual evaluation)
                import numpy as np
                np.random.seed(42)  # Reproducible results
                
                mock_metrics = {
                    'rmse': training_results.get('best_val_loss', 1.25) + np.random.normal(0, 0.02),
                    'precision_at_10': 0.30 + np.random.normal(0, 0.03),
                    'ndcg_at_10': 0.40 + np.random.normal(0, 0.02),
                    'diversity': 0.50 + np.random.normal(0, 0.05),
                    'coverage': 0.40 + np.random.normal(0, 0.04)
                }
                
                # Ensure realistic bounds
                mock_metrics['precision_at_10'] = max(0.15, min(0.50, mock_metrics['precision_at_10']))
                mock_metrics['ndcg_at_10'] = max(0.25, min(0.60, mock_metrics['ndcg_at_10']))
                mock_metrics['diversity'] = max(0.30, min(0.80, mock_metrics['diversity']))
                mock_metrics['coverage'] = max(0.20, min(0.70, mock_metrics['coverage']))
                
                # Track business metrics
                metadata = {
                    'model_name': model_name,
                    'github_sha': '${{ github.sha }}',
                    'training_date': training_results.get('training_date'),
                    'param_count': training_results.get('param_count')
                }
                
                business_record = tracker.track_variant_metrics(
                    variant_id=model_name,
                    ml_metrics=mock_metrics,
                    metadata=metadata
                )
                
                evaluation_results[model_name] = {
                    'ml_metrics': mock_metrics,
                    'business_metrics': business_record['business_metrics'],
                    'revenue_impact': business_record['revenue_impact'],
                    'training_info': training_results
                }
                
                print(f'   RMSE: {mock_metrics[\"rmse\"]:.4f}')
                print(f'   Precision@10: {mock_metrics[\"precision_at_10\"]:.3f}')
                print(f'   Revenue Impact: \${business_record[\"revenue_impact\"][\"net_annual_impact\"]:,.0f}')
                
            except Exception as e:
                print(f'‚ùå Evaluation failed for {model_file}: {e}')
                continue
        
        # Compare models and select winner
        if len(evaluation_results) > 1:
            print('\\nüèÜ Model Comparison:')
            comparison = tracker.compare_variants(list(evaluation_results.keys()))
            
            if 'winner_variant_id' in comparison:
                winner = comparison['winner_variant_id']
                winner_revenue = comparison['projected_annual_revenue_impact']
                improvement = comparison['business_value_improvement']
                
                print(f'   Winner: {winner}')
                print(f'   Revenue Impact: \${winner_revenue:,.0f}')
                print(f'   Improvement: {improvement:.1f}%')
                
                # Save comparison results
                with open('model_comparison_results.json', 'w') as f:
                    json.dump(comparison, f, indent=2, default=str)
                
                # Set output for next jobs
                with open('$GITHUB_OUTPUT', 'a') as f:
                    f.write(f'best_model={winner}\\n')
                    f.write(f'revenue_impact={winner_revenue:.0f}\\n')
                    f.write(f'should_deploy={str(improvement > 5.0).lower()}\\n')
        
        # Save detailed evaluation results
        with open('evaluation_results.json', 'w') as f:
            json.dump(evaluation_results, f, indent=2, default=str)
        
        print('\\n‚úÖ Model evaluation completed!')
        "
      id: evaluate
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: |
          evaluation_results.json
          model_comparison_results.json

  deploy-decision:
    runs-on: ubuntu-latest
    needs: [evaluate-models]
    if: always()
    
    steps:
    - name: Deployment decision
      run: |
        echo "ü§î Making deployment decision..."
        
        # In a real workflow, this would check model performance thresholds
        # and decide whether to trigger deployment
        
        echo "Model training pipeline completed successfully!"
        echo "‚úÖ Models ready for manual review and deployment"
        
        # Could trigger deployment workflow here based on conditions
        # if [[ "${{ needs.evaluate-models.outputs.should_deploy }}" == "true" ]]; then
        #   echo "üöÄ Triggering deployment workflow"
        # fi

  notify:
    runs-on: ubuntu-latest
    needs: [validate-data, train-models, evaluate-models, deploy-decision]
    if: always()
    
    steps:
    - name: Training pipeline summary
      run: |
        echo "üéØ Training Pipeline Summary"
        echo "=========================="
        echo "Data Validation: ${{ needs.validate-data.result }}"
        echo "Model Training: ${{ needs.train-models.result }}"
        echo "Model Evaluation: ${{ needs.evaluate-models.result }}"
        echo "Deploy Decision: ${{ needs.deploy-decision.result }}"
        echo ""
        
        if [[ "${{ needs.validate-data.result }}" == "success" && "${{ needs.train-models.result }}" == "success" ]]; then
          echo "‚úÖ Training pipeline completed successfully!"
          echo "üìä Data Quality Score: ${{ needs.validate-data.outputs.data_quality_score }}"
        else
          echo "‚ùå Training pipeline encountered issues"
        fi