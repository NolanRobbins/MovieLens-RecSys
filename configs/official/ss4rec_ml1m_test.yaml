# SS4Rec ML-1M Test Configuration
# Testing SS4Rec model on standard RecBole ml-1m dataset before using custom ml-25m
# Based on: "SS4Rec: Continuous-Time Sequential Recommendation with State Space Models"
# arXiv: https://arxiv.org/abs/2502.08132

# Model Configuration
model: SS4RecOfficial
dataset: ml-1m

# SS4Rec Architecture Parameters (from paper)
hidden_size: 64             # Model dimension
n_layers: 2                 # Number of SS4Rec layers
dropout_prob: 0.5           # Dropout probability
loss_type: 'BPR'           # Bayesian Personalized Ranking for sequential recommendation

# State Space Model Parameters
d_state: 16                 # State dimension for SSMs
d_conv: 4                   # Convolution dimension
expand: 2                   # Expansion factor
dt_min: 0.001              # Minimum discretization step
dt_max: 0.1                # Maximum discretization step

# Training Parameters (optimized for ml-1m - smaller dataset)
learning_rate: 0.001        # AdamW learning rate
train_batch_size: 2048      # Smaller batch size for ml-1m
eval_batch_size: 2048       # Evaluation batch size
epochs: 100                 # Fewer epochs for testing
stopping_step: 5            # Early stopping patience
weight_decay: 0.0001        # L2 regularization

# Evaluation Configuration
metrics: ['Recall', 'MRR', 'NDCG', 'Hit']    # Standard RecSys metrics
topk: [1, 5, 10, 20]                         # Top-K for evaluation
valid_metric: 'NDCG@10'                      # Primary validation metric

# Data Configuration (RecBole Standard for ml-1m)
USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
RATING_FIELD: rating
TIME_FIELD: timestamp
data_path: 'data/recbole_format'  # Relative path to RecBole format data directory
download: True  # Enable automatic download of ml-1m dataset
MAX_ITEM_LIST_LENGTH: 50    # Maximum sequence length
ITEM_LIST_LENGTH_FIELD: item_length
LIST_SUFFIX: _list
load_col:                   # Columns to load for ml-1m
  inter: ['user_id', 'item_id', 'rating', 'timestamp']
  
# Data filtering for ml-1m (standard RecBole settings)
user_inter_num_interval: "[10,inf)"
item_inter_num_interval: "[10,inf)"
val_interval:
  rating: "[3,inf)"
  timestamp: "[97830000, inf)"

# Data splitting (RecBole Standard)
eval_args:
  group_by: user
  split: {'RS': [0.8, 0.1, 0.1]}   # Random split for testing
  order: 'RO'                       # Random order for testing
  mode: full
  
# Device and Reproducibility  
device: cuda                # GPU training
gpu_id: 0                   # Use GPU 0
reproducibility: true       # Ensure reproducible results
seed: 2023                 # Random seed

# Single-GPU distributed training parameters (required by RecBole)
nproc: 1                    # Number of processes (single GPU)
world_size: 1               # Total number of processes
offset: 0                   # Group offset (starting rank)
ip: localhost               # Master node IP
port: 29500                 # Master node port
backend: gloo               # Use gloo backend for single-GPU

# Optimization
scheduler: 'StepLR'         # Learning rate scheduler
step_size: 50               # LR scheduler step size (adjusted for fewer epochs)
gamma: 0.5                  # LR decay factor

# Logging and Checkpointing
log_wandb: false           # Disable W&B for testing
save_dataset: false        # Don't save processed dataset
save_dataloaders: false    # Don't save data loaders
checkpoint_dir: 'results/official_ss4rec/ml1m_test/checkpoints'

# Performance Optimization
num_workers: 4              # Data loading workers  
pin_memory: true            # Pin memory for faster GPU transfer

# Expected Performance Targets for ml-1m:
# - HR@10: > 0.25 (lower than ml-25m due to smaller dataset)
# - NDCG@10: > 0.20  
# - MRR@10: Competitive with baselines
# - Training Time: 1-2 hours on A6000 GPU (much faster than ml-25m)
